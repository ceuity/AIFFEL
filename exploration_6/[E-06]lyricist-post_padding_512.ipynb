{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['\"Don\\'t worry about a thing,', \"'Cause every little thing gonna be all right.\", 'Singin\\': \"Don\\'t worry about a thing,']\n"
     ]
    }
   ],
   "source": [
    "#데이터 불러오기\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Don't worry about a thing,\n",
      "'Cause every little thing gonna be all right.\n",
      "Singin': \"Don't worry about a thing,\n",
      "'Cause every little thing gonna be all right!\" Rise up this mornin',\n",
      "Smiled with the risin' sun,\n",
      "Three little birds\n",
      "Perch by my doorstep\n",
      "Singin' sweet songs\n",
      "Of melodies pure and true,\n",
      "Sayin', (\"This is my message to you-ou-ou:\") Singin': \"Don't worry 'bout a thing,\n"
     ]
    }
   ],
   "source": [
    "# 문장 indexing\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 스킵\n",
    "\n",
    "    if idx > 9: break\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고 양쪽 공백을 삭제\n",
    "\n",
    "    # 정규식을 이용하여 문장 처리\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 패턴의 특수문자를 만나면 특수문자 양쪽에 공백을 추가\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 공백 패턴을 만나면 스페이스 1개로 치환\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿ 패턴을 제외한 모든 문자(공백문자까지도)를 스페이스 1개로 치환\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start> don t worry about a thing , <end>', '<start> cause every little thing gonna be all right . <end>']\n",
      "156013\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "# 모든 문장에 전처리 함수 적용\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0:\n",
    "        continue\n",
    "    if sentence[-1] == \":\":\n",
    "        continue\n",
    "    temp = preprocess_sentence(sentence)\n",
    "    if len(temp.split()) <= 15:\n",
    "        corpus.append(temp)\n",
    "        \n",
    "print(corpus[:2])\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<start>', 'don', 't', 'worry', 'about', 'a', 'thing', ',', '<end>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처리한 문장 확인\n",
    "print(corpus[0].split())\n",
    "len(corpus[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   40   16 ...    0    0    0]\n",
      " [   2   66  124 ...    0    0    0]\n",
      " [   2 1544   40 ...    0    0    0]\n",
      " ...\n",
      " [   2   38  905 ...    0    0    0]\n",
      " [   2   38   68 ...    0    0    0]\n",
      " [   2    8   83 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f55a8ecc090>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "def tokenize(corpus):\n",
    "    # 텐서플로우에서 제공하는 Tokenizer 패키지를 생성\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=15000,  # 전체 단어의 개수 \n",
    "        filters=' ',      # 전처리 로직\n",
    "        oov_token=\"<unk>\" # out-of-vocabulary, 사전에 없는 단어\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) # 우리가 구축한 corpus로부터 Tokenizer가 사전을 자동구축\n",
    "\n",
    "    # tokenizer를 활용하여 모델에 입력할 데이터셋을 구축\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # tokenizer는 구축한 사전으로부터 corpus를 해석해 Tensor로 변환\n",
    "\n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞추기 위한 padding 메소드\n",
    "    # maxlen의 디폴트값은 None입니다. 이 경우 corpus의 가장 긴 문장을 기준으로 시퀀스 길이가 맞춰진다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                           padding='post',\n",
    "                                                           maxlen=15)\n",
    "\n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(tensor[10,:])) # 생성된 텐서 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : that\n",
      "16 : t\n",
      "17 : s\n",
      "18 : on\n",
      "19 : your\n",
      "20 : of\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전의 index\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 20: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 생성\n",
    "src_input = tensor[:, :-1] # tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성. 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다.\n",
    "tgt_input = tensor[:, 1:]  # tensor에서 <start>를 잘라내서 타겟 문장을 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  40  16 700 111   9 183   5   3   0   0   0   0   0]\n",
      "[ 40  16 700 111   9 183   5   3   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 문장 확인\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test set 분리\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input,\n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "# 분리된 데이터 확인\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((512, 14), (512, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 구축\n",
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 512\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1    # 0:<pad>를 포함하여 dictionary 갯수 + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성 함수\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "embedding_size = 256 # 워드 벡터의 차원 수\n",
    "hidden_size = 1024 # LSTM Layer의 hidden 차원 수\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(512, 14, 15001), dtype=float32, numpy=\n",
       "array([[[-1.0224229e-05, -2.7752580e-04, -1.2825328e-04, ...,\n",
       "          4.4761804e-05,  1.0248564e-04,  1.9457337e-04],\n",
       "        [ 1.8985357e-04, -6.0376868e-04, -3.1143182e-04, ...,\n",
       "         -1.9653777e-04, -5.8816066e-05,  1.8992188e-04],\n",
       "        [ 1.8232486e-04, -8.4343180e-04, -6.2991597e-04, ...,\n",
       "         -2.5223842e-04, -2.2866552e-04,  1.7806405e-04],\n",
       "        ...,\n",
       "        [ 1.6102778e-03, -3.4189556e-04, -2.7676011e-04, ...,\n",
       "         -4.4959932e-04, -5.1134476e-04,  1.6483878e-04],\n",
       "        [ 1.6681906e-03, -4.6135817e-04, -1.9318514e-04, ...,\n",
       "         -6.6915312e-04, -7.2519149e-04,  4.3512366e-04],\n",
       "        [ 1.7134207e-03, -5.9933978e-04, -1.2384290e-04, ...,\n",
       "         -9.6014561e-04, -9.2771277e-04,  7.0822152e-04]],\n",
       "\n",
       "       [[-1.0224229e-05, -2.7752580e-04, -1.2825328e-04, ...,\n",
       "          4.4761804e-05,  1.0248564e-04,  1.9457337e-04],\n",
       "        [ 5.6094257e-05, -1.4613013e-04, -2.5617867e-04, ...,\n",
       "          8.5987231e-05,  2.0834323e-04,  4.2341519e-04],\n",
       "        [ 1.8775462e-04,  2.1127905e-04, -3.6746103e-04, ...,\n",
       "          1.3332354e-04,  2.8442120e-04,  6.1847293e-04],\n",
       "        ...,\n",
       "        [ 1.8522216e-03,  6.5568875e-04,  9.7594038e-04, ...,\n",
       "          8.9995988e-04,  1.9663091e-04,  1.5207092e-04],\n",
       "        [ 1.8353454e-03,  4.7128624e-04,  9.8947645e-04, ...,\n",
       "          7.8841840e-04, -4.8854417e-05,  2.5243772e-04],\n",
       "        [ 1.8328360e-03,  2.3744057e-04,  9.3386724e-04, ...,\n",
       "          5.6472170e-04, -3.3758231e-04,  4.3812144e-04]],\n",
       "\n",
       "       [[-1.0224229e-05, -2.7752580e-04, -1.2825328e-04, ...,\n",
       "          4.4761804e-05,  1.0248564e-04,  1.9457337e-04],\n",
       "        [-2.1117783e-04, -4.0435101e-04, -5.8228587e-05, ...,\n",
       "          8.7007968e-05,  3.1812579e-04,  3.5739026e-04],\n",
       "        [-4.3025162e-04, -3.2116714e-04, -1.9029988e-04, ...,\n",
       "         -7.7843324e-05,  5.5611000e-04,  1.0196011e-04],\n",
       "        ...,\n",
       "        [ 3.1750157e-04,  3.2029769e-04,  4.0675528e-04, ...,\n",
       "         -7.4371559e-07,  7.0027262e-04,  4.6095354e-04],\n",
       "        [ 5.2390504e-04,  9.2415212e-05,  3.9598852e-04, ...,\n",
       "         -1.5024442e-04,  3.6302124e-04,  7.1442942e-04],\n",
       "        [ 7.1647979e-04, -1.5912855e-04,  3.6319130e-04, ...,\n",
       "         -4.0064671e-04,  9.0040130e-06,  9.7663293e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.0224229e-05, -2.7752580e-04, -1.2825328e-04, ...,\n",
       "          4.4761804e-05,  1.0248564e-04,  1.9457337e-04],\n",
       "        [-3.5039320e-05, -4.9810600e-04, -2.2992976e-04, ...,\n",
       "          5.4336477e-05,  3.3316770e-04,  3.2545376e-04],\n",
       "        [-7.9822319e-05, -5.2973005e-04, -5.0859700e-04, ...,\n",
       "          1.1865445e-04,  6.5081485e-04,  4.0755927e-04],\n",
       "        ...,\n",
       "        [ 5.5168588e-05, -1.5142554e-04, -7.7835884e-04, ...,\n",
       "         -7.6697051e-04,  7.1173545e-04, -7.5915334e-05],\n",
       "        [-5.9920425e-05, -6.5580218e-07, -3.9041447e-04, ...,\n",
       "         -9.2203921e-04,  8.3181175e-04, -1.3757969e-04],\n",
       "        [ 8.2263279e-05, -5.5022181e-05, -1.0851491e-04, ...,\n",
       "         -1.0228707e-03,  7.1877782e-04, -5.3163931e-05]],\n",
       "\n",
       "       [[-1.0224229e-05, -2.7752580e-04, -1.2825328e-04, ...,\n",
       "          4.4761804e-05,  1.0248564e-04,  1.9457337e-04],\n",
       "        [ 1.8065066e-04, -3.0253269e-04, -2.1297554e-04, ...,\n",
       "          4.1885192e-05,  3.0539400e-04,  2.4351336e-04],\n",
       "        [ 2.9108743e-04, -3.4401176e-04, -2.3407937e-04, ...,\n",
       "          2.5045771e-05,  2.2471005e-04,  2.3829160e-04],\n",
       "        ...,\n",
       "        [-2.7377196e-04,  6.7345524e-04, -5.7947535e-05, ...,\n",
       "          1.0239361e-03, -6.1402842e-04, -1.0526868e-04],\n",
       "        [-1.8424973e-04,  6.2362198e-04,  2.4792840e-05, ...,\n",
       "          1.0199546e-03, -6.3369976e-04, -9.2129063e-05],\n",
       "        [ 3.3327473e-05,  4.4828467e-04,  8.5892614e-05, ...,\n",
       "          8.7458809e-04, -7.7623077e-04,  3.5384004e-05]],\n",
       "\n",
       "       [[-1.0224229e-05, -2.7752580e-04, -1.2825328e-04, ...,\n",
       "          4.4761804e-05,  1.0248564e-04,  1.9457337e-04],\n",
       "        [-6.7045134e-05, -4.4506177e-04, -2.2487326e-04, ...,\n",
       "          7.2359744e-06,  1.7832490e-04,  2.2543609e-04],\n",
       "        [ 4.8913156e-05, -5.5332528e-04, -3.4888188e-04, ...,\n",
       "         -1.9814572e-04,  1.7455957e-04,  2.0032772e-04],\n",
       "        ...,\n",
       "        [ 1.1459350e-03, -1.4458415e-03,  1.1557154e-05, ...,\n",
       "         -1.8760593e-03, -1.4298856e-03,  1.3346229e-03],\n",
       "        [ 1.2219881e-03, -1.5356879e-03,  1.6504662e-05, ...,\n",
       "         -2.3064206e-03, -1.5534339e-03,  1.5179716e-03],\n",
       "        [ 1.2910246e-03, -1.6187065e-03,  2.0815844e-05, ...,\n",
       "         -2.7247195e-03, -1.6428177e-03,  1.6655558e-03]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델의 데이터 확인\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3840256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  15376025  \n",
      "=================================================================\n",
      "Total params: 32,855,961\n",
      "Trainable params: 32,855,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델의 최종 출력 shape는 (256, 14, 15001)\n",
    "# 256은 batch_size, 14는 squence_length, 15001은 단어의 갯수(Dense Layer 출력 차원 수)\n",
    "\n",
    "model.summary() # sequence_length를 모르기 때문에 Output shape를 정확하게 모른다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "243/243 [==============================] - 61s 250ms/step - loss: 3.7809\n",
      "Epoch 2/30\n",
      "243/243 [==============================] - 61s 253ms/step - loss: 3.1885\n",
      "Epoch 3/30\n",
      "243/243 [==============================] - 62s 254ms/step - loss: 3.0280\n",
      "Epoch 4/30\n",
      "243/243 [==============================] - 62s 254ms/step - loss: 2.9146\n",
      "Epoch 5/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.8233\n",
      "Epoch 6/30\n",
      "243/243 [==============================] - 62s 256ms/step - loss: 2.7440\n",
      "Epoch 7/30\n",
      "243/243 [==============================] - 62s 254ms/step - loss: 2.6723\n",
      "Epoch 8/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.6068\n",
      "Epoch 9/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.5443\n",
      "Epoch 10/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.4858\n",
      "Epoch 11/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.4307\n",
      "Epoch 12/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.3782\n",
      "Epoch 13/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.3268\n",
      "Epoch 14/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.2788\n",
      "Epoch 15/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.2331\n",
      "Epoch 16/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.1886\n",
      "Epoch 17/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.1466\n",
      "Epoch 18/30\n",
      "243/243 [==============================] - 62s 255ms/step - loss: 2.1051\n",
      "Epoch 19/30\n",
      "243/243 [==============================] - 63s 259ms/step - loss: 2.0653\n",
      "Epoch 20/30\n",
      "243/243 [==============================] - 63s 258ms/step - loss: 2.0263\n",
      "Epoch 21/30\n",
      "243/243 [==============================] - 63s 259ms/step - loss: 1.9890\n",
      "Epoch 22/30\n",
      "243/243 [==============================] - 63s 259ms/step - loss: 1.9522\n",
      "Epoch 23/30\n",
      "243/243 [==============================] - 63s 258ms/step - loss: 1.9162\n",
      "Epoch 24/30\n",
      "243/243 [==============================] - 63s 258ms/step - loss: 1.8811\n",
      "Epoch 25/30\n",
      "243/243 [==============================] - 62s 257ms/step - loss: 1.8467\n",
      "Epoch 26/30\n",
      "243/243 [==============================] - 62s 257ms/step - loss: 1.8131\n",
      "Epoch 27/30\n",
      "243/243 [==============================] - 62s 257ms/step - loss: 1.7802\n",
      "Epoch 28/30\n",
      "243/243 [==============================] - 62s 257ms/step - loss: 1.7487\n",
      "Epoch 29/30\n",
      "243/243 [==============================] - 62s 257ms/step - loss: 1.7171\n",
      "Epoch 30/30\n",
      "243/243 [==============================] - 62s 257ms/step - loss: 1.6868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f55a6f74950>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 생성 함수\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어를 하나씩 생성\n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 입력받은 문장\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]   # 예측한 단어가 새로 생성된 단어 \n",
    "\n",
    "        # 우리 모델이 새롭게 예측한 단어를 입력 문장의 뒤에 붙이기\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 우리 모델이 <end>를 예측했거나, max_len에 도달하지 않았다면  while 루프를 또 돌면서 다음 단어를 예측\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # 생성된 tensor 안에 있는 word index를 tokenizer.index_word 사전을 통해 실제 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated   # 최종 생성된 자연어 문장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i m not gonna crack <end> '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회고록"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- range(n)도 reverse() 함수가 먹힌다는 걸 오늘 알았다...\n",
    "- 예시에 주어진 train data 갯수는 124960인걸 보면 총 데이터는 156200개인 것 같은데 아무리 전처리 단계에서 조건에 맞게 처리해도 168000개 정도가 나온다. 아무튼 일단 돌려본다.\n",
    "- 문장의 길이가 최대 15라는 이야기는 <start>, <end>를 포함하여 15가 되어야 하는 것 같아서 tokenize했을 때 문장의 길이가 13 이하인 것만 corpus로 만들었다.\n",
    "- 학습 회차 별 생성된 문장 input : `<start> i love`\n",
    "    - 1회차 `'<start> i love you , i love you <end> '`\n",
    "    - 2회차 `'<start> i love you , i m not gonna crack <end> '`\n",
    "    - 3회차`'<start> i love you to be a shot , i m not a man <end> '`\n",
    "    - 4회차 `'<start> i love you , i m not stunning , i m a fool <end> '`\n",
    "- batch_size를 각각 256, 512, 1024로 늘려서 진행했는데, 1epoch당 걸리는 시간이 74s, 62s, 59s 정도로 batch_size 배수 만큼의 차이는 없었다. batch_size가 배로 늘어나면 걸리느 시간도 당연히 반으로 줄어들 것이라 생각했는데 오산이었다.\n",
    "- 1회차는 tokenize 했을 때 length가 15 이하인 것을 train_data로 사용하였다.\n",
    "- 2, 3, 4회차는 tokenize 했을 때 length가 13 이하인 것을 train_data로 사용하였다.\n",
    "- 3회차는 2회차랑 동일한 데이터에 padding 을 post에서 pre로 변경하였다. RNN에서는 뒤에 padding을 넣는 것 보다 앞쪽에 padding을 넣어주는 쪽이 마지막 결과에 paddind이 미치는 영향이 적어지기 때문에 더 좋은 성능을 낼 수 있다고 알고있기 때문이다.\n",
    "- 근데 실제로는 pre padding 쪽이 loss가 더 크게 나왔다. 확인해보니 이론상으로는 pre padding이 성능이 더 좋지만 실제로는 post padding쪽이 성능이 더 잘 나와서 post padding을 많이 쓴다고 한다.\n",
    "- batch_size를 변경해서 pre padding을 한 번 더 돌려보았더니 같은 조건에서의 post padding 보다 loss가 높았고 문장도 부자연스러웠다. 앞으로는 post padding을 사용해야겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 훈련 파일   \n",
    "[lyricist-post_padding_256](./[E-06]lyricist-post_padding_256.ipynb)   \n",
    "[lyricist-post_padding_512](./[E-06]lyricist-post_padding_512.ipynb)   \n",
    "[lyricist-pre_padding_512](./[E-06]lyricist-pre_padding_512.ipynb)   \n",
    "[lyricist-pre_padding_1024](./[E-06]lyricist-pre_padding_512.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
